{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cec9d6f-e303-435d-af07-f2fe04a599ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d8edce9-ecac-43f3-97cf-9c34cae74997",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "from jaxtyping import Float, Integer, Bool\n",
    "from tinyshakespeare import Vocab, TinyShakespeareDataset, make_train_val_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a65aad27-cc07-4f08-97af-d218101bb7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# even though GPT is a decoder-only transformer, in PyTorch terminology\n",
    "# a decoder has a cross-attention component (which GPT does not);\n",
    "# therefore, we implement a \"decoder-only\" transformer using\n",
    "# PyTorch built-ins with the TransformerEncoderLayer and provide a\n",
    "# causal (upper-triangular) mask to the input\n",
    "\n",
    "# TODO: since vocab is part of the model (unfortunately?), put it in the GPT class\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        d_model: int = 128,\n",
    "        num_heads: int = 8,\n",
    "        d_ff: int = 256,\n",
    "        dropout: float = 0.\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=d_ff,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=6)\n",
    "        # swappable output \"head\" at the end of the encoders\n",
    "        self.output = nn.Sequential(\n",
    "            nn.Linear(d_model, vocab_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Integer[Tensor, \"batch tokens\"]):\n",
    "        _, t = x.shape\n",
    "        causal_mask = torch.triu(torch.ones(t, t, device=x.device), diagonal=1)\n",
    "        embed_o: Float[Tensor, \"batch tokens {d_model}\"] = self.embed(x)\n",
    "\n",
    "        enc_o = self.encoder(embed_o, mask=causal_mask, is_causal=True)\n",
    "        output_o = self.output(enc_o)\n",
    "\n",
    "        return output_o\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae3eabc6-be24-4177-b997-4569b654b51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(\n",
    "    model,\n",
    "    dl,\n",
    "    device: torch.device,\n",
    "    num_iters: int = 10\n",
    "):\n",
    "    losses = {\"train\": 0, \"val\": 0}\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    for split in (\"train\", \"val\"):\n",
    "        dl_iter = iter(dl[split])\n",
    "        \n",
    "        for _ in range(num_iters):\n",
    "            x, y = next(dl_iter)\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            \n",
    "            logits = model(x)\n",
    "            loss = F.cross_entropy(logits.transpose(-2, -1), y)\n",
    "            losses[split] += loss.item()\n",
    "\n",
    "        losses[split] /= num_iters\n",
    "        \n",
    "    return losses\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69e36328-91be-4926-bc59-74984ee33eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model: nn.Module,\n",
    "    dl: dict[str, DataLoader],\n",
    "    optimizer: optim.Optimizer,\n",
    "    steps: int = 100,\n",
    "    eval_interval: int = 10,\n",
    "    device: str | None = None,\n",
    "):\n",
    "    if device is None:\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    device = torch.device(device)\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    train_iter = iter(dl[\"train\"])\n",
    "    \n",
    "    for steps in range(1, steps + 1):\n",
    "        model.train()\n",
    "        \n",
    "        x, y = next(train_iter)\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        logits = model(x)\n",
    "        loss = F.cross_entropy(logits.transpose(-2, -1), y)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if steps % eval_interval == 0:\n",
    "            losses = estimate_loss(model, dl, device)\n",
    "            print(f\"train loss: {losses['train']:.4f} ; val loss: {losses['val']:.4f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36adfced-45ec-4c80-9b76-9243eadaf81d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 3.4424 ; val loss: 3.4715\n",
      "train loss: 3.3115 ; val loss: 3.3194\n",
      "train loss: 3.1665 ; val loss: 3.1910\n",
      "train loss: 3.0207 ; val loss: 3.0618\n",
      "train loss: 2.8966 ; val loss: 2.9075\n",
      "train loss: 2.8249 ; val loss: 2.8290\n",
      "train loss: 2.7897 ; val loss: 2.7467\n",
      "train loss: 2.6947 ; val loss: 2.6769\n",
      "train loss: 2.6245 ; val loss: 2.6123\n",
      "train loss: 2.5750 ; val loss: 2.6162\n",
      "train loss: 2.5730 ; val loss: 2.5613\n",
      "train loss: 2.5613 ; val loss: 2.5512\n",
      "train loss: 2.5431 ; val loss: 2.5477\n",
      "train loss: 2.5078 ; val loss: 2.5032\n",
      "train loss: 2.4721 ; val loss: 2.5028\n",
      "train loss: 2.4487 ; val loss: 2.5714\n",
      "train loss: 2.5039 ; val loss: 2.4845\n",
      "train loss: 2.4952 ; val loss: 2.4885\n",
      "train loss: 2.4755 ; val loss: 2.4428\n",
      "train loss: 2.4878 ; val loss: 2.4358\n",
      "train loss: 2.4566 ; val loss: 2.4913\n",
      "train loss: 2.4740 ; val loss: 2.3791\n",
      "train loss: 2.4309 ; val loss: 2.4159\n",
      "train loss: 2.4171 ; val loss: 2.3822\n",
      "train loss: 2.3556 ; val loss: 2.3672\n",
      "train loss: 2.3970 ; val loss: 2.4314\n",
      "train loss: 2.3474 ; val loss: 2.4020\n",
      "train loss: 2.3475 ; val loss: 2.4514\n",
      "train loss: 2.3996 ; val loss: 2.4161\n",
      "train loss: 2.4027 ; val loss: 2.3888\n",
      "train loss: 2.3942 ; val loss: 2.3849\n",
      "train loss: 2.3588 ; val loss: 2.3482\n",
      "train loss: 2.3765 ; val loss: 2.3687\n",
      "train loss: 2.3405 ; val loss: 2.3689\n",
      "train loss: 2.3258 ; val loss: 2.3516\n",
      "train loss: 2.2968 ; val loss: 2.4024\n",
      "train loss: 2.3350 ; val loss: 2.3469\n",
      "train loss: 2.3362 ; val loss: 2.3514\n",
      "train loss: 2.3226 ; val loss: 2.3094\n",
      "train loss: 2.3127 ; val loss: 2.3032\n",
      "train loss: 2.2582 ; val loss: 2.3388\n",
      "train loss: 2.3111 ; val loss: 2.3905\n",
      "train loss: 2.3419 ; val loss: 2.3373\n",
      "train loss: 2.3109 ; val loss: 2.3330\n",
      "train loss: 2.2939 ; val loss: 2.3318\n",
      "train loss: 2.2480 ; val loss: 2.3110\n",
      "train loss: 2.3222 ; val loss: 2.2632\n",
      "train loss: 2.2660 ; val loss: 2.2662\n",
      "train loss: 2.2930 ; val loss: 2.3351\n",
      "train loss: 2.2496 ; val loss: 2.2537\n",
      "train loss: 2.2537 ; val loss: 2.2944\n",
      "train loss: 2.2811 ; val loss: 2.3077\n",
      "train loss: 2.2174 ; val loss: 2.2806\n",
      "train loss: 2.2772 ; val loss: 2.3284\n",
      "train loss: 2.2745 ; val loss: 2.2329\n",
      "train loss: 2.2372 ; val loss: 2.2682\n",
      "train loss: 2.2222 ; val loss: 2.3473\n",
      "train loss: 2.2511 ; val loss: 2.2666\n",
      "train loss: 2.2600 ; val loss: 2.2955\n",
      "train loss: 2.2273 ; val loss: 2.2366\n",
      "train loss: 2.2663 ; val loss: 2.2789\n",
      "train loss: 2.1999 ; val loss: 2.2228\n",
      "train loss: 2.2411 ; val loss: 2.2378\n",
      "train loss: 2.2449 ; val loss: 2.3006\n",
      "train loss: 2.1913 ; val loss: 2.2380\n",
      "train loss: 2.2070 ; val loss: 2.2303\n",
      "train loss: 2.2342 ; val loss: 2.3035\n",
      "train loss: 2.2333 ; val loss: 2.2636\n",
      "train loss: 2.2325 ; val loss: 2.2752\n",
      "train loss: 2.2497 ; val loss: 2.2639\n",
      "train loss: 2.2322 ; val loss: 2.1931\n",
      "train loss: 2.1889 ; val loss: 2.2245\n",
      "train loss: 2.1910 ; val loss: 2.2095\n",
      "train loss: 2.2084 ; val loss: 2.2391\n",
      "train loss: 2.2145 ; val loss: 2.2074\n",
      "train loss: 2.1833 ; val loss: 2.2561\n",
      "train loss: 2.2309 ; val loss: 2.2351\n",
      "train loss: 2.1792 ; val loss: 2.2217\n",
      "train loss: 2.2423 ; val loss: 2.2003\n",
      "train loss: 2.2291 ; val loss: 2.2375\n",
      "train loss: 2.1957 ; val loss: 2.2266\n",
      "train loss: 2.1866 ; val loss: 2.2247\n",
      "train loss: 2.2277 ; val loss: 2.2236\n",
      "train loss: 2.1572 ; val loss: 2.2149\n",
      "train loss: 2.1677 ; val loss: 2.2664\n",
      "train loss: 2.1740 ; val loss: 2.2092\n",
      "train loss: 2.2107 ; val loss: 2.2128\n",
      "train loss: 2.0913 ; val loss: 2.2283\n",
      "train loss: 2.1313 ; val loss: 2.1803\n",
      "train loss: 2.2195 ; val loss: 2.1878\n",
      "train loss: 2.1495 ; val loss: 2.1962\n",
      "train loss: 2.1614 ; val loss: 2.1951\n",
      "train loss: 2.1776 ; val loss: 2.1782\n",
      "train loss: 2.1004 ; val loss: 2.2173\n",
      "train loss: 2.1185 ; val loss: 2.1754\n",
      "train loss: 2.1649 ; val loss: 2.1498\n",
      "train loss: 2.1844 ; val loss: 2.2335\n",
      "train loss: 2.1845 ; val loss: 2.1529\n",
      "train loss: 2.1229 ; val loss: 2.2122\n",
      "train loss: 2.1672 ; val loss: 2.1922\n"
     ]
    }
   ],
   "source": [
    "ds = TinyShakespeareDataset(\"./tinyshakespeare.txt\", seq_len=8)\n",
    "dl = make_train_val_dataloader(ds, batch_size=32, shuffle=True, num_workers=1)\n",
    "\n",
    "train_steps = 1000\n",
    "learning_rate = 3e-4\n",
    "\n",
    "model = GPT(\n",
    "    ds.vocab.size\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train(\n",
    "    model,\n",
    "    dl,\n",
    "    optimizer,\n",
    "    steps=train_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d1c5a27-fd50-4266-977f-beb8dd57f1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(\n",
    "    model: GPT,\n",
    "    vocab: Vocab,\n",
    "    prompt: str,\n",
    "    context_len: int = 8,\n",
    "    num_toks: int = 32,\n",
    "    device: str | None = None\n",
    "):\n",
    "    if device is None:\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    device = torch.device(device)\n",
    "\n",
    "    model.to(device)\n",
    "        \n",
    "    # batch size 1\n",
    "    ctx = torch.tensor([vocab.encode(list(prompt))]).to(device)\n",
    "\n",
    "    generated = []\n",
    "    \n",
    "    for i in range(num_toks):\n",
    "        # crop context to the context length\n",
    "        logits = model(ctx[:, -context_len:])\n",
    "        # we only care about the last token\n",
    "        logits = logits[:, -1, :]\n",
    "        # get probabilities\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        next_tok = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "        ctx = torch.cat((ctx, next_tok), dim=1)\n",
    "\n",
    "    return \"\".join(vocab.decode(ctx.tolist()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc93df8e-e4fa-421a-831f-a02882957e51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' may kind end him. fore noth Well'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(\n",
    "    model,\n",
    "    ds.vocab,\n",
    "    \" \"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d9f937-ec0e-4cc3-893c-b10d8145641a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xfm",
   "language": "python",
   "name": "xfm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
