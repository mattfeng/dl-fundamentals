{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cec9d6f-e303-435d-af07-f2fe04a599ea",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d8edce9-ecac-43f3-97cf-9c34cae74997",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "from jaxtyping import Float, Integer, Bool\n",
    "from tinyshakespeare import Vocab, TinyShakespeareDataset, make_train_val_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a65aad27-cc07-4f08-97af-d218101bb7bf",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false
   },
   "outputs": [],
   "source": [
    "# even though GPT is a decoder-only transformer, in PyTorch terminology\n",
    "# a decoder has a cross-attention component (which GPT does not);\n",
    "# therefore, we implement a \"decoder-only\" transformer using\n",
    "# PyTorch built-ins with the TransformerEncoderLayer and provide a\n",
    "# causal (upper-triangular) mask to the input\n",
    "\n",
    "# TODO: since vocab is part of the model (unfortunately?), put it in the GPT class\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        d_model: int = 128,\n",
    "        num_heads: int = 8,\n",
    "        d_ff: int = 256,\n",
    "        dropout: float = 0.\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=d_ff,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=6)\n",
    "        # swappable output \"head\" at the end of the encoders\n",
    "        self.output = nn.Sequential(\n",
    "            nn.Linear(d_model, vocab_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Integer[Tensor, \"batch tokens\"]):\n",
    "        _, t = x.shape\n",
    "        causal_mask = torch.triu(torch.ones(t, t, device=x.device), diagonal=1)\n",
    "        embed_o: Float[Tensor, \"batch tokens {d_model}\"] = self.embed(x)\n",
    "\n",
    "        enc_o = self.encoder(embed_o, mask=causal_mask, is_causal=True)\n",
    "        output_o = self.output(enc_o)\n",
    "\n",
    "        return output_o\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae3eabc6-be24-4177-b997-4569b654b51d",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(\n",
    "    model,\n",
    "    dl,\n",
    "    device: torch.device,\n",
    "    num_iters: int = 10\n",
    "):\n",
    "    losses = {\"train\": 0, \"val\": 0}\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    for split in (\"train\", \"val\"):\n",
    "        dl_iter = iter(dl[split])\n",
    "        \n",
    "        for _ in range(num_iters):\n",
    "            x, y = next(dl_iter)\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            \n",
    "            logits = model(x)\n",
    "            loss = F.cross_entropy(logits.transpose(-2, -1), y)\n",
    "            losses[split] += loss.item()\n",
    "\n",
    "        losses[split] /= num_iters\n",
    "        \n",
    "    return losses\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69e36328-91be-4926-bc59-74984ee33eda",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false
   },
   "outputs": [],
   "source": [
    "def train(\n",
    "    model: nn.Module,\n",
    "    dl: dict[str, DataLoader],\n",
    "    optimizer: optim.Optimizer,\n",
    "    steps: int = 100,\n",
    "    eval_interval: int = 10,\n",
    "    device: str | None = None,\n",
    "):\n",
    "    if device is None:\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    device = torch.device(device)\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    train_iter = iter(dl[\"train\"])\n",
    "    \n",
    "    for steps in range(1, steps + 1):\n",
    "        model.train()\n",
    "        \n",
    "        x, y = next(train_iter)\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        logits = model(x)\n",
    "        loss = F.cross_entropy(logits.transpose(-2, -1), y)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if steps % eval_interval == 0:\n",
    "            losses = estimate_loss(model, dl, device)\n",
    "            print(f\"train loss: {losses['train']:.4f} ; val loss: {losses['val']:.4f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36adfced-45ec-4c80-9b76-9243eadaf81d",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 2.5678 ; val loss: 2.5585\n",
      "train loss: 2.4736 ; val loss: 2.4889\n",
      "train loss: 2.4155 ; val loss: 2.4364\n",
      "train loss: 2.3475 ; val loss: 2.3924\n",
      "train loss: 2.3344 ; val loss: 2.3633\n",
      "train loss: 2.2978 ; val loss: 2.3191\n",
      "train loss: 2.2469 ; val loss: 2.2896\n",
      "train loss: 2.2135 ; val loss: 2.2354\n",
      "train loss: 2.1607 ; val loss: 2.1885\n",
      "train loss: 2.0921 ; val loss: 2.1706\n",
      "train loss: 2.0530 ; val loss: 2.1354\n",
      "train loss: 2.0336 ; val loss: 2.0939\n",
      "train loss: 1.9752 ; val loss: 2.0581\n",
      "train loss: 1.9553 ; val loss: 2.0205\n",
      "train loss: 1.9262 ; val loss: 2.0126\n",
      "train loss: 1.8869 ; val loss: 1.9971\n",
      "train loss: 1.8315 ; val loss: 1.9534\n",
      "train loss: 1.8292 ; val loss: 1.9429\n",
      "train loss: 1.8230 ; val loss: 1.9433\n",
      "train loss: 1.7753 ; val loss: 1.8985\n",
      "train loss: 1.7669 ; val loss: 1.8943\n",
      "train loss: 1.7491 ; val loss: 1.8890\n",
      "train loss: 1.7310 ; val loss: 1.8727\n",
      "train loss: 1.7328 ; val loss: 1.8659\n",
      "train loss: 1.6958 ; val loss: 1.8530\n",
      "train loss: 1.6882 ; val loss: 1.8279\n",
      "train loss: 1.6587 ; val loss: 1.8494\n",
      "train loss: 1.6439 ; val loss: 1.8071\n",
      "train loss: 1.6585 ; val loss: 1.8318\n",
      "train loss: 1.6312 ; val loss: 1.8168\n",
      "train loss: 1.6343 ; val loss: 1.7939\n",
      "train loss: 1.6144 ; val loss: 1.7909\n",
      "train loss: 1.6206 ; val loss: 1.7773\n",
      "train loss: 1.6017 ; val loss: 1.7763\n",
      "train loss: 1.5787 ; val loss: 1.7976\n",
      "train loss: 1.5946 ; val loss: 1.7814\n",
      "train loss: 1.5995 ; val loss: 1.7636\n",
      "train loss: 1.5764 ; val loss: 1.7639\n",
      "train loss: 1.5794 ; val loss: 1.7695\n",
      "train loss: 1.5415 ; val loss: 1.7597\n",
      "train loss: 1.5768 ; val loss: 1.7440\n",
      "train loss: 1.5460 ; val loss: 1.7314\n",
      "train loss: 1.5411 ; val loss: 1.7262\n",
      "train loss: 1.5388 ; val loss: 1.7425\n",
      "train loss: 1.5447 ; val loss: 1.7169\n",
      "train loss: 1.5475 ; val loss: 1.7205\n",
      "train loss: 1.5288 ; val loss: 1.7232\n",
      "train loss: 1.5330 ; val loss: 1.7022\n",
      "train loss: 1.5084 ; val loss: 1.7037\n",
      "train loss: 1.5204 ; val loss: 1.6867\n",
      "train loss: 1.5390 ; val loss: 1.6976\n",
      "train loss: 1.5141 ; val loss: 1.7015\n",
      "train loss: 1.4882 ; val loss: 1.6899\n",
      "train loss: 1.5171 ; val loss: 1.7134\n",
      "train loss: 1.4919 ; val loss: 1.7030\n",
      "train loss: 1.4881 ; val loss: 1.6829\n",
      "train loss: 1.4854 ; val loss: 1.7053\n",
      "train loss: 1.4822 ; val loss: 1.6758\n",
      "train loss: 1.4857 ; val loss: 1.6809\n",
      "train loss: 1.4854 ; val loss: 1.6875\n",
      "train loss: 1.4845 ; val loss: 1.6769\n",
      "train loss: 1.4968 ; val loss: 1.6823\n",
      "train loss: 1.4751 ; val loss: 1.6700\n",
      "train loss: 1.4536 ; val loss: 1.6822\n",
      "train loss: 1.4753 ; val loss: 1.6768\n",
      "train loss: 1.4654 ; val loss: 1.6677\n",
      "train loss: 1.4495 ; val loss: 1.6760\n",
      "train loss: 1.4794 ; val loss: 1.6581\n",
      "train loss: 1.4633 ; val loss: 1.6778\n",
      "train loss: 1.4610 ; val loss: 1.6679\n",
      "train loss: 1.4512 ; val loss: 1.6751\n",
      "train loss: 1.4534 ; val loss: 1.6640\n",
      "train loss: 1.4465 ; val loss: 1.6567\n",
      "train loss: 1.4318 ; val loss: 1.6550\n",
      "train loss: 1.4193 ; val loss: 1.6533\n",
      "train loss: 1.4235 ; val loss: 1.6330\n",
      "train loss: 1.4482 ; val loss: 1.6439\n",
      "train loss: 1.4382 ; val loss: 1.6662\n",
      "train loss: 1.4305 ; val loss: 1.6786\n",
      "train loss: 1.4207 ; val loss: 1.6329\n",
      "train loss: 1.4304 ; val loss: 1.6223\n",
      "train loss: 1.4273 ; val loss: 1.6525\n",
      "train loss: 1.4226 ; val loss: 1.6678\n",
      "train loss: 1.4169 ; val loss: 1.6645\n",
      "train loss: 1.4134 ; val loss: 1.6487\n",
      "train loss: 1.4146 ; val loss: 1.6089\n",
      "train loss: 1.4237 ; val loss: 1.6091\n",
      "train loss: 1.4096 ; val loss: 1.6587\n",
      "train loss: 1.3872 ; val loss: 1.6411\n",
      "train loss: 1.4057 ; val loss: 1.6392\n",
      "train loss: 1.4195 ; val loss: 1.6566\n",
      "train loss: 1.4101 ; val loss: 1.6383\n",
      "train loss: 1.3952 ; val loss: 1.6538\n",
      "train loss: 1.3849 ; val loss: 1.6271\n",
      "train loss: 1.4080 ; val loss: 1.5957\n",
      "train loss: 1.3948 ; val loss: 1.6383\n",
      "train loss: 1.3999 ; val loss: 1.6285\n",
      "train loss: 1.4062 ; val loss: 1.6608\n",
      "train loss: 1.3910 ; val loss: 1.5897\n",
      "train loss: 1.3886 ; val loss: 1.6293\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters\n",
    "\n",
    "seq_len = 128\n",
    "train_steps = 10000\n",
    "learning_rate = 3e-4\n",
    "\n",
    "ds = TinyShakespeareDataset(\"./data/tinyshakespeare.txt\", seq_len=seq_len)\n",
    "dl = make_train_val_dataloader(ds, batch_size=32, shuffle=True, num_workers=1)\n",
    "\n",
    "torch.manual_seed(38650)\n",
    "\n",
    "model = GPT(\n",
    "    ds.vocab.size\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train(\n",
    "    model,\n",
    "    dl,\n",
    "    optimizer,\n",
    "    steps=train_steps,\n",
    "    eval_interval=train_steps//100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d1c5a27-fd50-4266-977f-beb8dd57f1cc",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false
   },
   "outputs": [],
   "source": [
    "def generate(\n",
    "    model: GPT,\n",
    "    vocab: Vocab,\n",
    "    prompt: str,\n",
    "    context_len: int = 8,\n",
    "    num_toks: int = 32,\n",
    "    device: str | None = None\n",
    "):\n",
    "    if device is None:\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    device = torch.device(device)\n",
    "\n",
    "    model.to(device)\n",
    "        \n",
    "    # batch size 1\n",
    "    ctx = torch.tensor([vocab.encode(list(prompt))], dtype=torch.long).to(device)\n",
    "\n",
    "    generated = []\n",
    "    \n",
    "    for i in range(num_toks):\n",
    "        # crop context to the context length\n",
    "        logits = model(ctx[:, -context_len:])\n",
    "        # we only care about the last token\n",
    "        logits = logits[:, -1, :]\n",
    "        # get probabilities\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        next_tok = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "        ctx = torch.cat((ctx, next_tok), dim=1)\n",
    "\n",
    "    return \"\".join(vocab.decode(ctx.tolist()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc93df8e-e4fa-421a-831f-a02882957e51",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " not ground stap and made in me.\n",
      "No, there 'tis you near friar your death.\n",
      "\n",
      "KING RICHARD II:\n",
      "Ay, the doth is cheeks sit?\n",
      "\n",
      "QUEEN ELIZABETH:\n",
      "And here it be has relet you,\n",
      "There thy treason same where is me:\n",
      "Nor see it fengar them her is life,\n",
      "And marrings dead. 'But and ratisful, Thurse speed\n",
      "To sent thyself, and you life, be your very,\n",
      "Some welcome then.\n",
      "\n",
      "Both:\n",
      "'Tis open bruing for you, surple, and me me,\n",
      "The morning of your forfecialintess!\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "Fealines, come, so,\n",
      "May doubting so him nox'd way: \n"
     ]
    }
   ],
   "source": [
    "gen = generate(\n",
    "    model,\n",
    "    ds.vocab,\n",
    "    \" \",\n",
    "    context_len=seq_len,\n",
    "    num_toks=512\n",
    ")\n",
    "print(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961b3cf7-c4e5-4f84-bf80-87c55d3b52b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xfm",
   "language": "python",
   "name": "xfm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
