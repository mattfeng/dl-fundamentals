{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82b8467a-83f2-4c74-b52f-ef110e9ea120",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c57c4e-567d-425e-8684-e93ed5af8f67",
   "metadata": {
    "deletable": false,
    "editable": false,
    "frozen": true
   },
   "source": [
    "## lucidrain reference implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d203da63-0d82-4762-95f6-00f475e8cac5",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from rotary_embedding_torch import RotaryEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f012fee-a761-40db-b2f3-d6dd92a4b9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the positional embedding in your transformer and pass to all your attention layers\n",
    "rotary_emb = RotaryEmbedding(dim=6)\n",
    "\n",
    "# mock queries and keys - dimensions should end with (seq_len, feature dimension), and any number of preceding dimensions (batch, heads, etc)\n",
    "q = torch.randn(1, 2, 4, 12) # queries - (batch, heads, seq len, dimension of head)\n",
    "k = torch.randn(1, 2, 4, 12) # keys\n",
    "\n",
    "# apply the rotations to your queries and keys after the heads have been split out, but prior to the dot product and subsequent softmax (attention)\n",
    "q = rotary_emb.rotate_queries_or_keys(q)\n",
    "k = rotary_emb.rotate_queries_or_keys(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "986f9ae1-6bd8-41d4-8364-a17a906acd8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-1.8969,  1.0745, -2.2239,  1.4666,  1.2950, -1.2456,  1.3795,\n",
       "            0.7095,  2.0144,  0.7948, -0.7084, -1.2765],\n",
       "          [ 1.1270,  1.0651,  0.3239,  0.1056,  1.9981, -1.3074,  0.1018,\n",
       "            2.1613, -0.1483, -0.9301, -0.0268,  0.0557],\n",
       "          [ 0.3227, -2.7809,  1.6189,  0.9686, -0.4253, -0.4536,  0.0475,\n",
       "            2.1043,  0.2759,  0.1436, -0.3441,  0.9428],\n",
       "          [-0.2085, -2.7235, -1.2989, -1.5051,  1.3584, -0.2239, -0.7351,\n",
       "            1.0767,  0.0352, -0.9284,  0.1211,  0.0389]],\n",
       "\n",
       "         [[ 0.0166, -0.9077, -0.5015, -0.3079,  0.4930,  0.5106, -1.1624,\n",
       "           -0.3516,  0.4895,  1.4627,  0.7148,  0.8116],\n",
       "          [ 0.4963, -0.1889, -0.6433, -1.5047,  1.6173,  1.1865, -1.3461,\n",
       "           -0.7524, -1.3093, -0.6362,  1.3876, -1.4166],\n",
       "          [ 1.1522,  0.2302, -0.4255,  0.0638,  0.2951,  0.0834, -0.4875,\n",
       "           -0.6189, -1.1064, -1.0282, -1.6161,  0.9371],\n",
       "          [ 1.7370,  0.6069,  0.6355,  0.5500,  1.4786, -0.3189, -2.1820,\n",
       "           -0.6836, -0.4355, -1.7385,  0.5348, -1.1491]]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ff10e9-4c15-449f-afd9-d49f3f12f7e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basics",
   "language": "python",
   "name": "basics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
