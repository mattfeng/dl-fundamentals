{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed722ec5-c9c3-44ab-9a7c-ddd95abc6f34",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "from tinyshakespeare import Vocab, TinyShakespeareDataset, make_train_val_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0faeccf-0e6c-40f9-9c8d-661a8a9982b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wavenet import WaveNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffd80ce8-0581-4699-a575-265043422845",
   "metadata": {
    "deletable": true,
    "editable": true,
    "frozen": false
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(\n",
    "    model,\n",
    "    dl,\n",
    "    device: torch.device,\n",
    "    num_iters: int = 10\n",
    "):\n",
    "    losses = {\"train\": 0, \"val\": 0}\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    for split in (\"train\", \"val\"):\n",
    "        dl_iter = iter(dl[split])\n",
    "        \n",
    "        for _ in range(num_iters):\n",
    "            x, y = next(dl_iter)\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            \n",
    "            logits = model(x)\n",
    "            loss = F.cross_entropy(logits.transpose(-2, -1), y)\n",
    "            losses[split] += loss.item()\n",
    "\n",
    "        losses[split] /= num_iters\n",
    "        \n",
    "    return losses\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "deba7820-b930-4438-b929-33daf900fce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model: nn.Module,\n",
    "    dl: dict[str, DataLoader],\n",
    "    optimizer: optim.Optimizer,\n",
    "    steps: int = 100,\n",
    "    eval_interval: int = 10,\n",
    "    device: str | None = None,\n",
    "):\n",
    "    if device is None:\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    device = torch.device(device)\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    train_iter = iter(dl[\"train\"])\n",
    "    \n",
    "    for steps in range(1, steps + 1):\n",
    "        model.train()\n",
    "        \n",
    "        x, y = next(train_iter)\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        logits = model(x)\n",
    "        loss = F.cross_entropy(logits.transpose(-2, -1), y)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if steps % eval_interval == 0:\n",
    "            losses = estimate_loss(model, dl, device)\n",
    "            print(f\"train loss: {losses['train']:.4f} ; val loss: {losses['val']:.4f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10f7ded3-b9d2-494b-9d05-45f0e24820e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 3.0187 ; val loss: 3.0301\n",
      "train loss: 2.5260 ; val loss: 2.5498\n",
      "train loss: 2.3343 ; val loss: 2.3185\n",
      "train loss: 2.2204 ; val loss: 2.2174\n",
      "train loss: 2.1614 ; val loss: 2.1770\n",
      "train loss: 2.1007 ; val loss: 2.1328\n",
      "train loss: 2.0545 ; val loss: 2.1148\n",
      "train loss: 2.0402 ; val loss: 2.0824\n",
      "train loss: 2.0251 ; val loss: 2.0443\n",
      "train loss: 1.9771 ; val loss: 2.0410\n",
      "train loss: 1.9583 ; val loss: 2.0339\n",
      "train loss: 1.9390 ; val loss: 2.0264\n",
      "train loss: 1.9230 ; val loss: 2.0210\n",
      "train loss: 1.9011 ; val loss: 1.9830\n",
      "train loss: 1.8844 ; val loss: 1.9946\n",
      "train loss: 1.8672 ; val loss: 1.9905\n",
      "train loss: 1.8653 ; val loss: 1.9754\n",
      "train loss: 1.8471 ; val loss: 1.9402\n",
      "train loss: 1.8476 ; val loss: 1.9561\n",
      "train loss: 1.8406 ; val loss: 1.9376\n",
      "train loss: 1.8282 ; val loss: 1.9386\n",
      "train loss: 1.8150 ; val loss: 1.9530\n",
      "train loss: 1.8121 ; val loss: 1.9266\n",
      "train loss: 1.8052 ; val loss: 1.9377\n",
      "train loss: 1.7679 ; val loss: 1.9098\n",
      "train loss: 1.7721 ; val loss: 1.9269\n",
      "train loss: 1.7713 ; val loss: 1.9064\n",
      "train loss: 1.7716 ; val loss: 1.9236\n",
      "train loss: 1.7877 ; val loss: 1.9019\n",
      "train loss: 1.7406 ; val loss: 1.8912\n",
      "train loss: 1.7435 ; val loss: 1.8818\n",
      "train loss: 1.7503 ; val loss: 1.9134\n",
      "train loss: 1.7417 ; val loss: 1.9036\n",
      "train loss: 1.7468 ; val loss: 1.8978\n",
      "train loss: 1.7469 ; val loss: 1.8898\n",
      "train loss: 1.7213 ; val loss: 1.8996\n",
      "train loss: 1.7248 ; val loss: 1.8928\n",
      "train loss: 1.7154 ; val loss: 1.8792\n",
      "train loss: 1.7071 ; val loss: 1.8769\n",
      "train loss: 1.7060 ; val loss: 1.8890\n",
      "train loss: 1.7080 ; val loss: 1.8941\n",
      "train loss: 1.7060 ; val loss: 1.8760\n",
      "train loss: 1.7164 ; val loss: 1.8665\n",
      "train loss: 1.6941 ; val loss: 1.8851\n",
      "train loss: 1.7007 ; val loss: 1.8594\n",
      "train loss: 1.6919 ; val loss: 1.8838\n",
      "train loss: 1.6714 ; val loss: 1.8841\n",
      "train loss: 1.6870 ; val loss: 1.8718\n",
      "train loss: 1.6808 ; val loss: 1.8697\n",
      "train loss: 1.6685 ; val loss: 1.8569\n",
      "train loss: 1.6591 ; val loss: 1.8667\n",
      "train loss: 1.6626 ; val loss: 1.8602\n",
      "train loss: 1.6864 ; val loss: 1.8554\n",
      "train loss: 1.6602 ; val loss: 1.8500\n",
      "train loss: 1.6531 ; val loss: 1.8394\n",
      "train loss: 1.6486 ; val loss: 1.8459\n",
      "train loss: 1.6481 ; val loss: 1.8307\n",
      "train loss: 1.6642 ; val loss: 1.8399\n",
      "train loss: 1.6577 ; val loss: 1.8458\n",
      "train loss: 1.6586 ; val loss: 1.8440\n",
      "train loss: 1.6435 ; val loss: 1.8462\n",
      "train loss: 1.6476 ; val loss: 1.8684\n",
      "train loss: 1.6437 ; val loss: 1.8609\n",
      "train loss: 1.6498 ; val loss: 1.8601\n",
      "train loss: 1.6358 ; val loss: 1.8568\n",
      "train loss: 1.6203 ; val loss: 1.8555\n",
      "train loss: 1.6531 ; val loss: 1.8319\n",
      "train loss: 1.6205 ; val loss: 1.8239\n",
      "train loss: 1.6278 ; val loss: 1.8487\n",
      "train loss: 1.6300 ; val loss: 1.8379\n",
      "train loss: 1.6320 ; val loss: 1.8515\n",
      "train loss: 1.6131 ; val loss: 1.8150\n",
      "train loss: 1.6260 ; val loss: 1.8345\n",
      "train loss: 1.6275 ; val loss: 1.8204\n",
      "train loss: 1.6172 ; val loss: 1.8242\n",
      "train loss: 1.6200 ; val loss: 1.8200\n",
      "train loss: 1.6155 ; val loss: 1.8494\n",
      "train loss: 1.6027 ; val loss: 1.8287\n",
      "train loss: 1.6026 ; val loss: 1.8098\n",
      "train loss: 1.6204 ; val loss: 1.8160\n",
      "train loss: 1.6072 ; val loss: 1.8066\n",
      "train loss: 1.6103 ; val loss: 1.8100\n",
      "train loss: 1.6079 ; val loss: 1.7962\n",
      "train loss: 1.6149 ; val loss: 1.8171\n",
      "train loss: 1.5868 ; val loss: 1.8120\n",
      "train loss: 1.6086 ; val loss: 1.8102\n",
      "train loss: 1.5777 ; val loss: 1.8343\n",
      "train loss: 1.5837 ; val loss: 1.7996\n",
      "train loss: 1.5997 ; val loss: 1.8252\n",
      "train loss: 1.5823 ; val loss: 1.8284\n",
      "train loss: 1.5975 ; val loss: 1.8175\n",
      "train loss: 1.5841 ; val loss: 1.8105\n",
      "train loss: 1.5973 ; val loss: 1.7924\n",
      "train loss: 1.5713 ; val loss: 1.8155\n",
      "train loss: 1.5966 ; val loss: 1.8174\n",
      "train loss: 1.5839 ; val loss: 1.8147\n",
      "train loss: 1.6010 ; val loss: 1.7954\n",
      "train loss: 1.5804 ; val loss: 1.8058\n",
      "train loss: 1.5751 ; val loss: 1.8037\n",
      "train loss: 1.5957 ; val loss: 1.8096\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters\n",
    "\n",
    "seq_len = 128\n",
    "train_steps = 10000\n",
    "learning_rate = 3e-4\n",
    "\n",
    "ds = TinyShakespeareDataset(\"./data/tinyshakespeare.txt\", seq_len=seq_len)\n",
    "dl = make_train_val_dataloader(ds, batch_size=32, shuffle=True, num_workers=1)\n",
    "\n",
    "torch.manual_seed(38650)\n",
    "\n",
    "model = WaveNet(\n",
    "    ds.vocab.size,\n",
    "    depth=8,\n",
    "    conv_dim=64,\n",
    "    residual_dim=64,\n",
    "    skip_dim=32,\n",
    "    head_dim=32\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train(\n",
    "    model,\n",
    "    dl,\n",
    "    optimizer,\n",
    "    steps=train_steps,\n",
    "    eval_interval=train_steps//100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24331168-9164-4081-96ed-c2d23f069aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(\n",
    "    model: nn.Module,\n",
    "    vocab: Vocab,\n",
    "    prompt: str,\n",
    "    context_len: int = 8,\n",
    "    num_toks: int = 32,\n",
    "    device: str | None = None\n",
    "):\n",
    "    if device is None:\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    device = torch.device(device)\n",
    "\n",
    "    model.to(device)\n",
    "        \n",
    "    # batch size 1\n",
    "    ctx = torch.tensor([vocab.encode(list(prompt))], dtype=torch.long).to(device)\n",
    "\n",
    "    generated = []\n",
    "    \n",
    "    for i in range(num_toks):\n",
    "        # crop input sequence to context length\n",
    "        logits = model(ctx[:, -context_len:])\n",
    "        # we only care about the last token\n",
    "        logits = logits[:, -1, :]\n",
    "        # get probabilities\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        next_tok = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "        ctx = torch.cat((ctx, next_tok), dim=1)\n",
    "\n",
    "    return \"\".join(vocab.decode(ctx.tolist()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de9b6d84-d7b5-47d0-b7fc-bfc9d1cc0be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " have so'erant poor-known in me villor\n",
      "services you men I'ring you telling stack upon bles, ond against will,\n",
      "He lessings,\n",
      "When thighbusings and him!\n",
      "\n",
      "Thirse relear,\n",
      "With your no trumned\n",
      "Witken flalive in, from will he gain morch your life,\n",
      "A crate, or Vould.\n",
      "\n",
      "BENVOLIO:\n",
      "His thou much of worrust Cant?\n",
      "Timpen i' son; it but hope therefore boince with heart shall you to be brucious,\n",
      "My upon you.\n",
      "Pavoam.\n",
      "\n",
      "AUTOLYCUS:\n",
      "\n",
      "KING EDWARD IV:\n",
      "Within eye!\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "Feam I sree\n",
      "Lends no and it mine you bollow\n",
      "Glood m\n"
     ]
    }
   ],
   "source": [
    "gen = generate(\n",
    "    model,\n",
    "    ds.vocab,\n",
    "    \" \",\n",
    "    context_len=seq_len,\n",
    "    num_toks=512\n",
    ")\n",
    "print(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e8e9bc-43cd-492a-ac98-483cc1e001ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wavenet",
   "language": "python",
   "name": "wavenet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
